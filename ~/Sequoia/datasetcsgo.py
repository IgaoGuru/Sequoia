import pandas as pd
import numpy as np
import os
from PIL import Image
from torch.utils.data import Dataset
import torch

img_size = (1280, 720)
TEAM_TR = 2
TEAM_CT = 3

def update_dataset(dataset_root_path, session_name, dict_dataset):
    """Receives a session folder and updates de dataset's dictionary with one keypair for each image;

    Note that the function does not return anything. It merely appends new keys to the received dictionary.

    Args:
        dataset_root_path (str): absolute path to directory containing session folder
        session_name (str): session folder's name
        dict_dataset (dict): dataset to be updated

    """

    #get annotation dataframe (df)
    annotation_path = f"{dataset_root_path}\\{session_name}\\annotation_{session_name}.txt"
    names = ['img','team','enemy','x0','y0','x1','y1']
    df = pd.read_csv(annotation_path, names=names)

    labels = np.empty((len(df), )).astype(str)
    #list where true indexes are ct
    ct_idxs = ((df.team == TEAM_TR) & (df.enemy == 1)) | ((df.team == TEAM_CT) & (df.enemy == 0))
    #list where tru indexes  are tr
    tr_idxs = ((df.team == TEAM_CT) & (df.enemy == 1)) | ((df.team == TEAM_TR) & (df.enemy == 0))
    labels[ct_idxs] = 'ct'
    labels[tr_idxs] = 'tr'

    df['label'] = labels
    df['key'] = [(session_name, str(img)) for img in df['img']] 

    #each group contains all bboxes and labels pertaining to the key's image
    gp_key = df[['key','label','x0','y0','x1','y1']].groupby(['key'])

    for key, df_key in gp_key:
        dict_dataset[key] = (df_key['label'].values, (df_key[['x0','y0','x1','y1']].values).astype(np.short))

# class CsgoDataset(Dataset):
class  CsgoDataset(Dataset):
    """Dataset based on dictionary where each screenshot corresponds to a key, and each bbox corresponds to key.

    This dataset is generated by combining all session folders in a given directory into a single dictionary.  
    Each key is a tuple containing [1] the name of the image's session, [2] the image's name.

    Each value is a numpy array of size 2 where each of it's values have two arrays of size n,
    where n is the number of bboxes that the key's image has. The first array contains the labels of
    its respective bboxes, and the second array contains the bboxes themselves.  
    For example, if an image has 3 bboxes, the value from the key:value pair may be:
    (('ct', 'tr', 'tr'), ((bbox1)(bbox2)(bbox3)))
    where 'ct'corresponds to bbox1, the first 'tr' to bbox2 and the second 'tr' to bbox3

    Args:
        dataset_root_path (str): absolute path to directory containing session folder
        session_name (str): session folder's name
        dict_dataset (dict): dataset to be updated

    """

    KNOWN_CLASSES = [
        "tr",
        "ct"
    ]

    def __init__(self, root_path, classes=None, transform=None, scale_factor=None):
        """generates dictionary

        Args:
            root_path (str): root directory that contains the session folders.
            classes (list): list of classes for NN classification.
            transform (list): pytorch transforms to be made to dataset's images.
            scale_factor (float): factor for image re-scaling. Defaults to 1.0.

        """
        self.dict_dataset = {}
        self.root_path = root_path
        self.transform = transform
        self.scale_factor = scale_factor
        if classes is None:
            self.classes = self.KNOWN_CLASSES
        else:
            self.classes = classes
        for _, dirs, _ in os.walk(root_path):
            for current_dir in dirs:
                update_dataset(self.root_path, current_dir, self.dict_dataset)
            break
        if len(self.dict_dataset) == 0:
            raise Exception('No dataset folder was found!')
        self.frame_keys = list(self.dict_dataset.keys())
        self.length = len(self.frame_keys)
        # print(self.dict_dataset)
        print(f'dataset\'s length is: {self.length} images')

    def __len__(self):
        return self.length

    def __str__(self):
        return self.root_path + " " + str(self.length)

    def __getitem__(self, idx):
        img = self.get_image(idx)
        if self.transform:
            img = self.transform(img)
        #clazz = self.dict_frames[self.frame_keys[idx]][0]

        bboxes = torch.tensor(self.dict_dataset[self.frame_keys[idx]][1], dtype=torch.float)
        if self.scale_factor != None:
            bboxes = bboxes * self.scale_factor
        labels = torch.tensor([self.classes.index(c) for c in self.dict_dataset[self.frame_keys[idx]][0]], dtype=torch.int64)
        return img, bboxes, labels

    def get_image_path(self, idx):
        """returns an image's absolute path based on its index in the dict_dataset keys.

        Args:
            idx (int): index for image in the dict_dataset keys.
        Returns:
            the image's absolute path as a string
        """
        return os.path.join(self.root_path, self.frame_keys[idx][0],'imgs',self.frame_keys[idx][1]+'.png') 

    def get_image(self, idx):
        """returns an image's matrix without alpha channel based on its index in the dict_dataset keys.

        Args:
            idx (int): index for image in the dict_dataset keys.
        Returns:
            the image's absolute path as a string
        """
        img_path = self.get_image_path(idx)
        img = Image.open(img_path).convert('RGB')
        return img

    def split(self, train, val, seed=None):
        """return random split of the dataset into training_set, validation_set, and test_set.

        Args:
            train (int): percentage of dataset to be allocated to training.
            val (float): percentage of dataset to be allocated to validation.
            seed (int): seed for random split.
        Returns:
            random split of the dataset into training_set, validation_set, and test_set.
        """
        if seed:
            torch.random.manual_seed(seed)
        train_size = int(train * len(self))
        val_size = int(val * len(self))
        test_size = len(self) - (train_size + val_size)
        return torch.utils.data.random_split(self, [train_size, val_size, test_size])